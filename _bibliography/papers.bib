---
---

@string{aps = {American Physical Society,}}

@incollection{TheodorouAlerTubella2023,
    author={Theodorou, Andreas and {Aler Tubella}, Andrea},
    title={Responsible {AI} at Work: Incorporating human values},      
    booktitle = {Handbook of Artificial Intelligence at Work: Interconnections and policy implications},
    year={In Press},
    editor={Garcia-Murillo, M., Renda, A., and MacInnes, I.},
    keywords={Governance, Policy, Framework, Human Control}
}

@inproceedings{MethnaniEtAl2023XAI3,
    author={Methnani, Leila and Dignum, Virginia and Theodorou, Andreas},
    title={Clash of the Explainers: Argumentation for Context-Appropriate Explanations},
    year={2023},
    booktitle = {XAI^3 Workshop at ECAI 2023},
    keywords={XAI, Framework, Argumentation}
}

@inproceedings{ramesh2023robot,
      title={Robot Health Indicator: A Visual Cue to Improve Level of Autonomy Switching Systems}, 
      author={Ramesh, Aniketh and Englund, Madeleine and Theodorou, Andreas and Stolkin, Rustam and Chiou, Manolis},
      year={2023},
      arxivId={2303.06776},
      booktitle = {Workshop on Variable Autonomy Teaming at HRI 2023},
      keywords={Transparency, Variable Autonomy, Human Control}
}

@inproceedings{TheodorouEtAl2022AIofAI,
    author={Theodorou, Andreas and Nieves, {Juan Carlos} and Dignum, Virginia},
    title={Good {AI} for Good: How {AI} Strategies of the Nordic Countries Address the Sustainable Development Goals},      
    booktitle = {AIofAI 2022: 2nd Workshop on Adverse Impacts and Collateral Effects of {AI} Technologies at IJCAI 2022},
    year={2022},
    arxivId={2210.09010},
    keywords={Governance, Policy, Framework}
}

@inproceedings{Methnani1753631,
   author = {Methnani, Leila and Br{\"a}nnstr{\"o}m, Mattias and Theodorou, Andreas},
   booktitle = {Human-Centered Artificial Intelligence : Advanced Lectures},
   institution = {Umeå University, Department of Computing Science},
   note = {Part of the book sub series: Lecture Notes in Artificial Intelligence (LNAI)Conference series: ACAI: ECCAI Advanced Course on Artificial Intelligence},
   pages = {304--321},
   title = {Operationalising AI ethics : conducting socio-technical assessment},
   series = {Lecture Notes in Computer Science},
   number = {13500},
   DOI = {10.1007/978-3-031-24349-3_16},
   keywords = {AI ethics, Responsible AI, Socio-technical assessment},
   abstract = {Several high profile incidents that involve Artificial Intelligence (AI) have captured public attention and increased demand for regulation. Low public trust and attitudes towards AI reinforce the need for concrete policy around its development and use. However, current guidelines and standards rolled out by institutions globally are considered by many as high-level and open to interpretation, making them difficult to put into practice. This paper presents ongoing research in the field of Responsible AI and explores numerous methods of operationalising AI ethics. If AI is to be effectively regulated, it must not be considered as a technology alone—AI is embedded in the fabric of our societies and should thus be treated as a socio-technical system, requiring multi-stakeholder involvement and employment of continuous value-based methods of assessment. When putting guidelines and standards into practice, context is of critical importance. The methods and frameworks presented in this paper emphasise this need and pave the way towards operational AI ethics. },
   ISBN = {9783031243486},
   year = {2023},
   keywords={Governance, Policy, Framework, Pedagogy}
}

@article{Baum1768532,
   author = {Baum, Kevin and Bryson, Joanna and Dignum, Frank and Dignum, Virginia and Grobelnik, Marko and Hoos, Holger and Irgens, Morten and Lukowicz, Paul and Muller, Catelijne and Rossi, Francesca and Shawe-Taylor, John and Theodorou, Andreas and Vinuesa, Ricardo},
   journal = {Frontiers in Computer Science},
   eid = {1210421},
   title = {From fear to action : AI governance and opportunities for all},
   volume = {5},
   DOI = {10.3389/fcomp.2023.1210421},
   keywords = {Artificial Intelligence, generative AI, governance, large language models, responsible AI, Trustworthy AI},
   year = {2023},
  keywords={Governance, Policy}
}

@inproceedings{BrannstromEtAl2022AISafety,
    author={Brännström, Mattias and Theodorou, Andreas and Dignum, Virginia},
    title={Let it{RAIN} for Social Good},      
    booktitle = {IJCAI 2022 Workshop on AISafety},
    year={2022},
    arxivId={2208.04697},
    keywords={Governance, Human Control, Accountability, Transparency, Policy, Framework},
}

@inproceedings{MethnaniEtAl2022COINE,
    author={Methnani, Leila and Antoniades, Andreas and Theodorou, Andreas},
    title={Embracing {AWKWARD}! Real-time Adjustment of Reactive Planning Using Social Norms},      
    booktitle = {Proceedings of the Coordination, Organizations, Institutions, Norms, and Ethics for Governance of Multi-Agent Systems (COINE) 2022},
    year={2022},
    keywords={Systems AI, Cognitive Architectures, Normative Agents, Reactive Planning},
}

@article{SartoriTheodorou2022,
    author={Sartori, Laura and Theodorou, Andreas},
    title={A sociotechnical perspective for the future of AI: narratives, inequalities, and human control},      
    journal={Ethics of Information Technology},
    year={2022},
    doi={10.1007/s10676-022-09624-3},
    URL={https://link.springer.com/article/10.1007/s10676-022-09624-3},
    volume={24},
    keywords={Transparency, Accountability, Human Control, Fairness, Narratives},
}

@article{Methnani2021Frontiers,
    author={Methnani, Leila and {Aler Tubella}, Andrea and Dignum, Virginia and Theodorou, Andreas},
    title={Let Me Take Over: Variable Autonomy For Meaningful Human Control},      
    journal={Frontiers in Artificial Intelligence},  
    year={2021},
    volume={4},
    pages={133},
    URL={https://www.frontiersin.org/article/10.3389/frai.2021.737072},
    doi={10.3389/frai.2021.737072},
    ISSN={2624-8212},   
    keywords = {Systems AI, Transparency, Accountability, Variable Autonomy, Human Control},
}

@article{WinfieldEtAl2021Frontiers,
    author={Winfield, {Alan F. T}. and Booth, {Serena} and Dennis, {Louise A.} and Egawa, Takashi and Hastie, Helen and Jacobs, Naomi and Muttram, {Roderick I.} and Olszewska, {Joanna I.} and Rajabiyazdi, Fahimeh and Theodorou, Andreas and Underwood, {Mark A.} and Wortham, {Robert H.} and Watson, Eleanor},   
    title={IEEE P7001: A Proposed Standard on Transparency},      
    journal={Frontiers in Robotics and AI},
    volume={8},
    issue={665729},
    year={2021},
    url={https://www.frontiersin.org/article/10.3389/frobt.2021.665729},       
    doi={10.3389/frobt.2021.665729},      
    ISSN={2296-9144},   
    abstract={This paper describes IEEE P7001, a new draft standard on transparency of autonomous systems<xref ref-type="fn" rid="fn1"><sup>1</sup></xref>. In the paper, we outline the development and structure of the draft standard. We present the rationale for transparency as a measurable, testable property. We outline five stakeholder groups: users, the general public and bystanders, safety certification agencies, incident/accident investigators and lawyers/expert witnesses, and explain the thinking behind the normative definitions of “levels” of transparency for each stakeholder group in P7001. The paper illustrates the application of P7001 through worked examples of both specification and assessment of fictional autonomous systems.},
    keywords = {Transparency, Accountability, Verification, Governance, Policy, Framework}
}

@inproceedings{AlerTubellaEtAl2021AAMAS,
    author = {{Aler Tubella}, Andrea and Theodorou, Andreas and Nieves, {Juan Carlos}},
    title = {Interrogating the Black Box: Transparency through Information-Seeking Dialogues},
    year = {2021},
    abstract = {This paper is preoccupied with the following question: given a (possibly opaque) learning
	system, how can we understand whether its behaviour adheres to governance constraints?
	The answer can be quite simple: we just need to "ask" the system about it. We propose
	to construct an investigator agent to query a learning agent-- the suspect agent--
	to investigate its adherence to a given ethical policy in the context of an information-seeking
	dialogue, modeled in formal argumentation settings. This formal dialogue framework
	is the main contribution of this paper. Through it, we break down compliance checking
	mechanisms into three modular components, each of which can be tailored to various
	needs in a vast amount of ways: an investigator agent, a suspect agent, and an acceptance
	protocol determining whether the responses of the suspect agent comply with the policy.
	This acceptance protocol presents a fundamentally different approach to aggregation:
	rather than using quantitative methods to deal with the non-determinism of a learning
	system, we leverage the use of argumentation semantics to investigate the notion of
	properties holding consistently. Overall, we argue that the introduced formal dialogue
	framework opens many avenues both in the area of compliance checking and in the analysis
	of properties of opaque systems.},
    booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
    pages = {106–114},
    numpages = {9},
    keywords = {Systems AI, Transparency, Accountability, Verification, Glass Box, Formal},
    doi = {10.5555/3463952.3463971},
    arxivId = {2102.04714}
 }
 
@techreport{AI4EUD54,
    title={AI4EU Deliverable 5.4: A Software Engineering Methodology},
    author={{Ul Islam}, Zahoor and Theodorou, Andreas and Nieves, {Juan Carlos} and Dignum, Virginia},
    year={2020},
    keywords = {Policy, Framework}
 }
 
@techreport{MullerEtAl2020Analysis,
    title={Final Anaysis on the EU Whitepaper on AI},
    author={Catelijne Muller and Virginia Dignum  and Andreas Theodorou},
    year={2020},
    url = {https://tinyurl.com/yc6bw9bf},
    keywords = {Governance, Policy, Framework}
 }

@inproceedings{AlerTubellaEtAl2020RULEML,
	author = {{Aler Tubella}, Andrea and Theodorou, Andreas and Dignum, Virginia and Michael, Loizos},
	booktitle = {4th International Joint Conference on Rules and Reasoning (RuleML)},
	month = jun,
	address = {Olso, Norway},
	title = {{Contestable Black Boxes}},
	arxivId = {2006.05133},
	keywords = {XAI, Transparency, HRI, Governance},
	doi = {10.1007/978-3-030-57977-7_12},
	year = {2020}
}

@article{VinuesaEtAl2020SocioTechnical,
    title={A socio-technical framework for digital contact tracing},
    author={Ricardo Vinuesa and Andreas Theodorou and Manuela Battaglini and Virginia Dignum},
    year={2020},
    journal = {Results in Engineering},
    keywords = {Governance, Policy, Framework},
    arxivId = {2005.08370},
    doi = {10.1016/j.rineng.2020.100163}
 }

@article{TheodorouDignum2020NMI,
	author = {Theodorou, Andreas and Dignum, Virginia},
	year = {2020},
	doi = {10.1038/s42256-019-0136-y6},
	journal = {Nature Machine Intelligence},
	title = {{Towards ethical and socio-legal governance in AI}},
	volume = {2},
	issue = {1},
	keywords = {Governance, Policy, Whitepaper},
	url = {{https://www.nature.com/articles/s42256-019-0136-y.epdf?shared_access_token=tLQjOyEeJJBLNYBpYAddZ9RgN0jAjWel9jnR3ZoTv0OKielulWtfIKdoTkc7o23A4ag4RzhLocCFIkMqRYeFumYGAnLqPSfK_tQ3761isKFC32POZ17DGXFsQMNDEcD8X2AnDXspfKQtpudOtnxcvQ%3D%3D}}
}

@incollection{Theodorou2020AIReflections,
	author = {Theodorou, Andreas},
	booktitle = {Artificial Intelligence: Reflections in Philosophy, Theology, and Social Sciences},
	editor = {Goecke, Benedikt Paul  and Rosenthal-von der P{\"{u}}tten, Astrid Marieke},
	publisher = {Brill},
	title = {{Why Artificial Intelligence is a matter of Design}},
	keywords = {Moral Agency, Transparency, Cognitive Science},
	doi = {10.30965/9783957437488_009},
	year = {2020}
}

@incollection{BrysonTheodorou2019,
	abstract = {Although not a universally held goal, maintaining human-centric artificial intelligence is necessary for society's long-term stability. Fortunately, the legal and technological problems of maintaining control are actually fairly well understood and amenable to engineering. The real problem is establishing the social and political will for assigning and maintaining accountability for artifacts when these artefacts are generated or used. In this chapter we review the necessity and tractability of maintaining human control, and the mechanisms by which this can be achieved. What makes the problem both most interesting and most threatening is that achieving consensus around such an approach requires at least some measure of agreement on broad existential concerns.},
	author = {Bryson, Joanna J and Theodorou, Andreas},
	booktitle = {Human-Centered Digitalization and Services},
	doi = {10.1007/978-981-13-7725-9_16},
	editor = {Toivonen-Noro, Marja and Saari, Evelina and Melkas, Helin{\"{a}} and Hasu, Mervin},
	pages = {305--323},
	publisher = {Springer},
	title = {{How Society Can Maintain Human-Centric Artificial Intelligence}},
	url = {http://www.cs.bath.ac.uk/{~}jjb/web/publications.html http://link.springer.com/10.1007/978-981-13-7725-9{\_}16},
	keywords = {Systems AI, Cognitive Architectures, Transparency, Accountability, Human Control},
	year = {2019}
}
@inproceedings{RotsidisEtAl2019ROMAN,
	address = {New Delhi, India},
	author = {Rotsidis, Alexandros and Theodorou, Andreas and Bryson, Joanna J. and Wortham, Robert H.},
	booktitle = {2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
	doi = {10.1109/RO-MAN46459.2019.8956390},
	isbn = {978-1-7281-2622-7},
	month = oct,
	pages = {1--8},
	publisher = {IEEE},
	title = {{Improving Robot Transparency: An Investigation With Mobile Augmented Reality}},
	url = {https://ieeexplore.ieee.org/document/8956390/},
	keywords = {User Studies, Transparency, HRI, Trust, AR/VR/MR, Robotics},
	year = {2019}
}
@inproceedings{Theodorou2017EUCog,
	author = {Theodorou, Andreas},
	booktitle = {CEUR Workshop Proceedings},
	issn = {16130073},
	pages = {60--61},
	title = {{ABOD3: A graphical visualisation and real-time debugging tool for bod agents}},
	volume = {1855},
	year = {2016}
}
@phdthesis{Theodorou2019Thesis,
	abstract = {Transparency is a key consideration for the ethical design and use of Artificial Intelligence, and has recently become a topic of considerable public interest and debate. We frequently use philosophical, mathematical, and biologically inspired techniques for building artificial, interactive, intelligent agents. Yet despite these well-motivated inspirations, the resulting intelligence is often developed as a black box, communicating no understanding of how the underlying real-time decision making functions. This compromises both the safety of such systems and fair attribution of moral responsibility and legal accountability when incidents occur. This dissertation provides the knowledge and software tools to make artificially intelligent agents more transparent, allowing a direct understanding of the action-selection system of such a system. The use of transparency, as demonstrated in this document, helps not only with the debugging of intelligent agents, but also with the public's understanding of Artificial Intelligence (AI) by removing the 'scary' mystery around "why is it behaving like that". In the research described in this document I investigate and compare the perception we have of intelligent systems, such as robots and autonomous vehicles, when they are treated as black boxes compared to when we make their action-selection systems transparent. Finally, I make normative and descriptive arguments for the moral status of intelligent systems and contribute to regulatory policy regarding such systems.},
	author = {Theodorou, Andreas},
	url = {{https://github.com/RecklessCoding/recklesscoding.github.io/raw/master/files/andreasTheodorouThesis.pdf}},
	school = {University of Bath},
	title = {{AI Governance Through a Transparency Lens}},
	keywords = {User Studies, Systems AI, Cognitive Architectures, XAI, Transparency, Accountability, Governance, Policy, HRI, Human Control, Trust, Robotics},
	year = {2019}
}

@inproceedings{Theodorou2019COG,
	author = {Theodorou, Andreas and Bandt-law, Bryn and Bryson, Joanna J},
	booktitle = {IEEE Conference on Games},
	title = {{The Sustainability Game : AI Technology as an Intervention for Public Understanding of Cooperative Investment}},
	url = {https://bibbase.org/service/mendeley/3de73d64-9180-31af-9719-79efb2b3294b/file/7295b6eb-5102-39b9-0730-fdd46d5c2553/TheodorouEtAl{\_}TheSustainabilityGame{\_}COG2019{\_}SemiFinal.pdf.pdf},
	keywords = {User Studies, Games AI, Serious Games, ABM, Cognitive Science},
	year = {2019}
}

@article{Theodorou2017ConnectionScience,
	abstract = {The EPSRC's Principles of Robotics advises the implementation of transparency in robotic systems, however research related to AI transparency is in its infancy. This paper introduces the reader of the importance of having transparent inspection of intelligent agents and provides guidance for good practice when developing such agents. By considering and expanding upon other prominent definitions found in literature, we provide a robust definition of transparency as a mechanism to expose the decision-making of a robot. The paper continues by addressing potential design decisions developers need to consider when designing and developing transparent systems. Finally, we describe our new interactive intelligence editor, designed to visualise, develop and debug real-time intelligence.},
	author = {Theodorou, Andreas and Wortham, Robert H. and Bryson, Joanna J.},
	doi = {10.1080/09540091.2017.1310182},
	issn = {13600494},
	journal = {Connection Science},
	keywords = {Transparency, Accountability, Trust, Whitepaper},
	number = {3},
	pages = {230--241},
	title = {{Designing and implementing transparency for real time inspection of autonomous robots}},
	url = {https://researchportal.bath.ac.uk/files/154473870/TheodorouDesigningAndImplementingTransparency.pdf},
	volume = {29},
	year = {2017}
}

@inproceedings{AlerTubellaEtAl2019IJCAI,
	abstract = {Artificial Intelligence (AI) applications are being used to predict and assess behaviour in multiple domains, such as criminal justice and consumer finance, which directly affect human well-being. However, if AI is to improve people's lives, then people must be able to trust AI, which means being able to understand what the system is doing and why. Even though transparency is often seen as the requirement in this case, realistically it might not always be possible or desirable, whereas the need to ensure that the system operates within set moral bounds remains. In this paper, we present an approach to evaluate the moral bounds of an AI system based on the monitoring of its inputs and outputs. We place a "glass box" around the system by mapping moral values into explicit verifiable norms that constrain inputs and outputs, in such a way that if these remain within the box we can guarantee that the system adheres to the value. The focus on inputs and outputs allows for the verification and comparison of vastly different intelligent systems; from deep neural networks to agent-based systems. The explicit transformation of abstract moral values into concrete norms brings great benefits in terms of explainability; stakeholders know exactly how the system is interpreting and employing relevant abstract moral human values and calibrate their trust accordingly. Moreover, by operating at a higher level we can check the compliance of the system with different interpretations of the same value. These advantages will have an impact on the well-being of AI systems users at large, building their trust and providing them with concrete knowledge on how systems adhere to moral values.},
	archivePrefix = {arXiv},
	arxivId = {1905.04994},
	author = {{Aler Tubella}, Andrea  and Theodorou, Andreas and Dignum, Virginia and Dignum, Frank},
	booktitle = {International Joint Conference on Artificial Intellignece (IJCAI)},
	eprint = {1905.04994},
	title = {{Governance by Glass-Box: Implementing Transparent Moral Bounds for AI Behaviour}},
	keywords = {Systems AI, Transparency, Accountability, Verification, Glass Box, Formal},
	url = {http://arxiv.org/abs/1905.04994},
	year = {2019}
}

@inproceedings{WilsonTheodorou2019,
	address = {Macao, China},
	author = {Wilson, Holly and Theodorou, Andreas},
	booktitle = {IJCAI 2019 Workshop on AISafety},
	title = {{Slam the Brakes: Perceptions of Moral Decisions in Driving Dilemmas}},
	url = {{http://ceur-ws.org/Vol-2419/paper_14.pdf}},
	keywords = {User Studies, Transparency, HRI, Trust, Moral Agency, AR/VR/MR},
	year = {2019}
}

@article{Wortham2017ConnScience,
	abstract = {As robot reasoning becomes more complex, debugging becomes increasingly hard based solely on observable behaviour, even for robot designers and technical specialists. Similarly, non-specialist users find it hard to create useful mental models of robot reasoning solely from observed behaviour. The EPSRC Principles of Robotics mandate that our artefacts should be transparent, but what does this mean in practice, and how does transparency affect both trust and utility? We investigate this relationship in the literature and find it to be complex, particularly in non industrial environments where transparency may have a wider range of effects on trust and utility depending on the application and purpose of the robot. We outline our programme of research to support our assertion that it is nevertheless possible to create transparent agents that are emotion-ally engaging despite having a transparent machine nature.},
	author = {Wortham, Robert H. and Theodorou, Andreas},
	doi = {10.1080/09540091.2017.1313816},
	issn = {13600494},
	journal = {Connection Science},
	keywords = {EPOR,agents,ethics,roboethics,robotics,transparency},
	month = jul,
	number = {3},
	pages = {242--248},
	title = {{Robot transparency, trust and utility}},
	url = {https://researchportal.bath.ac.uk/files/154037267/transparency.pdf},
	keywords = {Moral Agency, Transparency, Whitepaper},
	volume = {29},
	year = {2017}
}

@inproceedings{Wortham2017ROMAN,
	author = {Wortham, Robert H. and Theodorou, Andreas and Bryson, Joanna J.},
	booktitle = {RO-MAN 2017 - 26th IEEE International Symposium on Robot and Human Interactive Communication},
	doi = {10.1109/ROMAN.2017.8172491},
	isbn = {9781538635186},
	issn = {1528-1132 0009-921X},
	month = aug,
	pages = {1424--1431},
	publisher = {IEEE},
	title = {{Improving robot transparency: Real-Time visualisation of robot AI substantially improves understanding in naive observers}},
	url = {https://researchportal.bath.ac.uk/files/155724003/robot{\_}transparency{\_}experiment.pdf},
	volume = {2017-Janua},
	keywords = {User Studies, Transparency, HRI, Robotics},
	year = {2017}
}

@article{WorthamEtAl2017TAROS,
	abstract = {Autonomous robots can be dicult to design and understand. Designers have diculty decoding the behaviour of their own robots simply by observing them. Naive users of robots similarly have diculty decoding robot behaviour simply through observation. In this paper we review relevant robot systems architecture, design and transparency literature, and report on a programme of research to investigate practical approaches to improve robot transparency. We report on the investigation of real-time graphical and vocalised outputs as a means for both designers and end users to gain a better mental model of the internal state and decision making processes taking place within a robot. This approach, combined with a graphical approach to behaviour design, o ers improved transparency for robot designers. We also report on studies of users' understanding, where significant improvement has been achieved using both graphical and vocalisation transparency approaches.},
	author = {Wortham, Robert H. and Theodorou, Andreas and Bryson, Joanna J.},
	doi = {10.1007/978-3-319-64107-2_22},
	isbn = {9783319641065},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {ABOD3,Arduino,Behaviour oriented design,Instinct planner,POSH,Reactive planning,Robot transparency},
	pages = {274--289},
	title = {{Robot transparency: Improving understanding of intelligent behaviour for designers and users}},
	volume = {10454 LNAI},
	keywords = {User Studies, Transparency, HRI, Robotics},
	year = {2017}
}

@inproceedings{WorthamEtAl2016IJCAI,
	abstract = {Deciphering the behaviour of intelligent others is a fundamental characteristic of our own intelligence. As we interact with complex intelligent artefacts, humans inevitably construct mental models to understand and predict their behaviour. If these models are incorrect or inadequate, we run the risk of self deception or even harm. This paper reports progress on a programme of work investigating approaches for implementing robot transparency, and the effects of these approaches on utility, trust and the perception of agency. Preliminary findings indicate that building transparency into robot action selection can help users build a more accurate understanding of the robot.},
	author = {Wortham, Robert H. and Theodorou, Andreas and Bryson, Joanna J.},
	booktitle = {IJCAI 2016 Ethics for Artificial Intelligence Workshop},
	title = {{What Does the Robot Think? Transparency as a Fundamental Design Requirement for Intelligent Systems}},
	url = {https://researchportal.bath.ac.uk/en/publications/what-does-the-robot-think-transparency-as-a-fundamental-design-re},
	year = {2016},
	keywords = {User Studies, Transparency, HRI, Robotics},
	address = {New York, NY, US}
}

@inproceedings{RotsidisEtAl2019IUI,
	title = {Robots that make sense: Transparent intelligence through augmented reality},
	year = {2019},
	keywords = {Artificial intelligence,Mobile augmented reality,Robots,Transparency},
	volume = {2327},
	address = {Los Angeles, CA USA},
	abstract = {Autonomous robots can be difficult to understand by their develop-ers, let alone by end users. Yet, as they become increasingly integralparts of our societies, the need for affordable easy to use tools toprovide transparency grows. The rise of the smartphone and theimprovements in mobile computing performance have graduallyallowed Augmented Reality (AR) to become more mobile and afford-able. In this paper we review relevant robot systems architectureand propose a new software tool to provide robot transparencythrough the use of AR technology. Our new tool, ABOD3-AR pro-vides real-time graphical visualisation and debugging of a robot’sgoals and priorities as a means for both designers and end usersto gain a better mental model of the internal state and decisionmaking processes taking place within a robot. We also report onour on-going research programme and planned studies to furtherunderstand the effects of transparency to naive users and experts.},
	author = {Rotsidis, Alexandros and Theodorou, Andreas and Wortham, Robert H.},
	keywords = {User Studies, Transparency, HRI, AR/VR/MR, Robotics},
	booktitle = {IUI'19 Workshop on Intelligent User Interfaces for Algorithmic Transparency in Emerging Technologies}
}

